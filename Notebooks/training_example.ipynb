{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f453f5",
   "metadata": {},
   "source": [
    "# Full Test of imported model\n",
    "\n",
    "Here we run through a full test of the imported model, including loading the model, defining the input data, and running the model to get the output. We will also include some additional checks to ensure the model is working correctly. This is a hard task to train on and it's worth noting that the model will not appear to be working correctly AT ALL. \n",
    "\n",
    "In order to convince you that the model is actually working, try out the blinking dataset, which has neurons that are blinking at different frequencies. Furthermore if you believe that the problem is that the model does diffusion, there is a secret ```disable_diffusion``` flag that you can use to disable the diffusion process. I don't want people shooting themselves in the foot so the flag must be enabled manually after a ```DifusssionWrapper``` is instantiated.\n",
    "\n",
    "Disabling Weights and Biases logging requires setting it to false in ```prepare_data_and_model``` and ```train_epoch```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573adca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sandman.data_loading.data_loader_map import make_map_loader\n",
    "from sandman.data_loading.chaotic_rnn_loader import make_chaotic_rnn_loader\n",
    "from sandman.data_loading.blinking import make_blinking_toy_loader\n",
    "from sandman.data_loading.data_loader_ibl import make_ibl_loader\n",
    "from sandman.paths import DATA_DIR\n",
    "\n",
    "from sandman.models.training import train_epoch, prepare_data_and_model\n",
    "\n",
    "from sandman.models.utils import TargetSpec, MaskingPolicy\n",
    "from sandman.models.utils import SpikeCountPoissonTarget, SpikeCountMSETarget, mask_one_region, mask_one_region_some_times, no_mask_policy\n",
    "\n",
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b3c61",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "Only uncomment one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264e597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data session  0\n",
      "Loading existing data session  1\n",
      "Loading existing data session  2\n",
      "Loading existing data session  3\n",
      "Loading existing data session  4\n",
      "num_neurons:  [174, 157, 104, 112, 138]\n",
      "num_trials:  {'train': [167, 165, 147, 136, 172], 'val': [56, 55, 49, 46, 58], 'test': [56, 55, 49, 46, 58]}\n",
      "Succesfully constructing the dataloader for  train\n",
      "Succesfully constructing the dataloader for  val\n",
      "Succesfully constructing the dataloader for  test\n",
      "Number of neurons: [174, 157, 104, 112, 138]\n"
     ]
    }
   ],
   "source": [
    "#========================== Import of MAP data loading\n",
    "\n",
    "# session_order = pickle.load(open(DATA_DIR / \"tables_and_infos/session_order.pkl\", \"rb\"))\n",
    "# eids = np.sort(session_order[:5]) # originall :40\n",
    "# print(\"Using eids:\", eids)\n",
    "\n",
    "# data_loader, num_neurons, datasets, areaoi_ind, area_ind_list_list, heldout_info_list, trial_type_dict = make_map_loader(\n",
    "#     eids,\n",
    "#     batch_size=2,\n",
    "#     include_opto=False\n",
    "# )\n",
    "\n",
    "# ========================== Import of IBL data loading\n",
    "\n",
    "# with open(DATA_DIR / \"tables_and_infos/ibl_eids.txt\") as file:\n",
    "#     eids = [line.rstrip() for line in file]\n",
    "\n",
    "# eids = eids[:2]\n",
    "# data_loader, num_neurons, datasets, areaoi_ind, area_ind_list_list, heldout_info_list, trial_type_dict = make_ibl_loader(\n",
    "#     eids,\n",
    "#     batch_size=1)\n",
    "\n",
    "#========================== Import of Chaotic RNN data loading\n",
    "\n",
    "eids = np.arange(5)\n",
    "data_loader, num_neurons, _, area_ind_list_list, record_info_list = make_chaotic_rnn_loader(\n",
    "    eids,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "#========================== Import of Blinking Toy data loading\n",
    "\n",
    "# data_loader = make_blinking_toy_loader(T=100, batch_size=1, device=\"mps\")\n",
    "# num_neurons = [2, 2, 4]\n",
    "\n",
    "#========================== Print number of neurons\n",
    "\n",
    "print(\"Number of neurons:\", num_neurons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3125d",
   "metadata": {},
   "source": [
    "### Preview of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c77e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes_data: torch.Size([1, 400, 157])\n",
      "The first (at most) 10 elements along each dimension of the spikes_data tensor are:\n",
      "tensor([[[ 4.,  1.,  0.,  0.,  3.,  0.,  0., 15.,  2.,  2.],\n",
      "         [ 5.,  0.,  0.,  0.,  1.,  0.,  0., 16.,  2.,  1.],\n",
      "         [ 3.,  0.,  1.,  1.,  3.,  0.,  1., 10.,  4.,  2.],\n",
      "         [ 7.,  1.,  1.,  1.,  3.,  0.,  0., 14.,  5.,  3.],\n",
      "         [ 4.,  2.,  0.,  0.,  4.,  1.,  0.,  5.,  3.,  2.],\n",
      "         [ 4.,  1.,  1.,  0.,  1.,  0.,  0., 10.,  3.,  4.],\n",
      "         [ 6.,  1.,  0.,  0.,  0.,  1.,  0., 13.,  2.,  0.],\n",
      "         [ 3.,  1.,  1.,  0.,  0.,  0.,  1., 12.,  4.,  0.],\n",
      "         [ 7.,  0.,  2.,  0.,  1.,  0.,  1., 14., 13.,  3.],\n",
      "         [ 6.,  0.,  3.,  0.,  0.,  1.,  2., 17.,  8.,  0.]]])\n",
      "\n",
      "spikes_timestamps: torch.Size([1, 400])\n",
      "The first (at most) 10 elements along each dimension of the spikes_timestamps tensor are:\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "\n",
      "target: torch.Size([1, 400, 1])\n",
      "The first (at most) 10 elements along each dimension of the target tensor are:\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "\n",
      "neuron_regions: torch.Size([1, 157])\n",
      "The first (at most) 10 elements along each dimension of the neuron_regions tensor are:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "eid: torch.Size([1])\n",
      "The first (at most) 10 elements along each dimension of the eid tensor are:\n",
      "tensor([1])\n",
      "\n",
      "fr: torch.Size([1, 400, 188])\n",
      "The first (at most) 10 elements along each dimension of the fr tensor are:\n",
      "tensor([[[ 5.8562,  1.0270,  0.4958,  0.2319,  2.7411,  0.0844,  0.1944,\n",
      "          11.7797,  2.3853,  1.2060],\n",
      "         [ 6.0113,  1.0050,  0.5308,  0.1923,  2.4923,  0.0786,  0.2064,\n",
      "          12.3952,  2.8764,  1.1704],\n",
      "         [ 6.1038,  0.9720,  0.5633,  0.1655,  2.2182,  0.0746,  0.2290,\n",
      "          12.8935,  3.4413,  1.1225],\n",
      "         [ 6.1535,  0.9276,  0.5955,  0.1477,  1.9252,  0.0723,  0.2653,\n",
      "          13.2869,  4.0937,  1.0611],\n",
      "         [ 6.1754,  0.8732,  0.6308,  0.1365,  1.6243,  0.0715,  0.3179,\n",
      "          13.5817,  4.8419,  0.9866],\n",
      "         [ 6.1783,  0.8127,  0.6727,  0.1301,  1.3293,  0.0721,  0.3887,\n",
      "          13.7771,  5.6757,  0.9009],\n",
      "         [ 6.1661,  0.7521,  0.7255,  0.1273,  1.0546,  0.0740,  0.4782,\n",
      "          13.8686,  6.5583,  0.8074],\n",
      "         [ 6.1394,  0.6978,  0.7937,  0.1274,  0.8128,  0.0771,  0.5850,\n",
      "          13.8520,  7.4287,  0.7106],\n",
      "         [ 6.0968,  0.6551,  0.8822,  0.1300,  0.6122,  0.0815,  0.7062,\n",
      "          13.7252,  8.2147,  0.6156],\n",
      "         [ 6.0356,  0.6273,  0.9954,  0.1347,  0.4552,  0.0871,  0.8373,\n",
      "          13.4862,  8.8491,  0.5271]]])\n",
      "\n",
      "factors: torch.Size([1, 400, 1000])\n",
      "The first (at most) 10 elements along each dimension of the factors tensor are:\n",
      "tensor([[[1.0440, 1.4510, 0.8232, 0.6710, 0.1849, 0.8395, 1.1062, 0.5263,\n",
      "          0.2960, 0.3480],\n",
      "         [1.0311, 1.5180, 0.8987, 0.6911, 0.2215, 0.8281, 1.1836, 0.5543,\n",
      "          0.2703, 0.3089],\n",
      "         [1.0173, 1.5738, 0.9689, 0.7079, 0.2681, 0.8150, 1.2620, 0.5803,\n",
      "          0.2526, 0.2798],\n",
      "         [1.0054, 1.6187, 1.0322, 0.7166, 0.3261, 0.8002, 1.3359, 0.6023,\n",
      "          0.2421, 0.2600],\n",
      "         [0.9977, 1.6534, 1.0881, 0.7131, 0.3969, 0.7844, 1.4014, 0.6190,\n",
      "          0.2376, 0.2488],\n",
      "         [0.9959, 1.6792, 1.1364, 0.6944, 0.4818, 0.7692, 1.4563, 0.6299,\n",
      "          0.2377, 0.2450],\n",
      "         [1.0008, 1.6969, 1.1775, 0.6600, 0.5813, 0.7561, 1.4997, 0.6354,\n",
      "          0.2407, 0.2474],\n",
      "         [1.0127, 1.7069, 1.2123, 0.6117, 0.6950, 0.7464, 1.5313, 0.6360,\n",
      "          0.2454, 0.2547],\n",
      "         [1.0312, 1.7091, 1.2425, 0.5530, 0.8203, 0.7412, 1.5511, 0.6325,\n",
      "          0.2508, 0.2656],\n",
      "         [1.0549, 1.7032, 1.2701, 0.4891, 0.9528, 0.7413, 1.5590, 0.6255,\n",
      "          0.2567, 0.2790]]])\n",
      "\n",
      "spikes_data_full: torch.Size([1, 400, 188])\n",
      "The first (at most) 10 elements along each dimension of the spikes_data_full tensor are:\n",
      "tensor([[[ 4.,  1.,  0.,  0.,  3.,  0.,  0., 15.,  2.,  2.],\n",
      "         [ 5.,  0.,  0.,  0.,  1.,  0.,  0., 16.,  2.,  1.],\n",
      "         [ 3.,  0.,  1.,  1.,  3.,  0.,  1., 10.,  4.,  2.],\n",
      "         [ 7.,  1.,  1.,  1.,  3.,  0.,  0., 14.,  5.,  3.],\n",
      "         [ 4.,  2.,  0.,  0.,  4.,  1.,  0.,  5.,  3.,  2.],\n",
      "         [ 4.,  1.,  1.,  0.,  1.,  0.,  0., 10.,  3.,  4.],\n",
      "         [ 6.,  1.,  0.,  0.,  0.,  1.,  0., 13.,  2.,  0.],\n",
      "         [ 3.,  1.,  1.,  0.,  0.,  0.,  1., 12.,  4.,  0.],\n",
      "         [ 7.,  0.,  2.,  0.,  1.,  0.,  1., 14., 13.,  3.],\n",
      "         [ 6.,  0.,  3.,  0.,  0.,  1.,  2., 17.,  8.,  0.]]])\n",
      "\n",
      "neuron_regions_full: torch.Size([1, 188])\n",
      "The first (at most) 10 elements along each dimension of the neuron_regions_full tensor are:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "trial_type: torch.Size([1])\n",
      "The first (at most) 10 elements along each dimension of the trial_type tensor are:\n",
      "tensor([0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(data_loader['train']))\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"{key}: {value.shape}\")\n",
    "    print(f\"The first (at most) 10 elements along each dimension of the {key} tensor are:\")\n",
    "    new_shape = tuple(slice(0, min(dim, 10)) for dim in value.shape)\n",
    "    print(value[new_shape])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7f46e",
   "metadata": {},
   "source": [
    "### Training/Model Objective!\n",
    "\n",
    "I'm proud of these. This is a generalization of the neural activation modelling process. Play around with the multitude I made in the ```sandman.models.utils``` module. Make your own too! They only take a couple of lines to make!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9045fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_spec = SpikeCountPoissonTarget(key=\"spikes_data\")\n",
    "masking_policy = no_mask_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de52067",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Good luck! It's tough to make something that works well. You are going to have a lot of ```n_neurons x d_model``` matrices for encoding and decoding so parameter count is mostly dependent on ```d_model```. Don't let it drop below 16 or it will fundementally break RoPE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5e84bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoah\u001b[0m to \u001b[32mhttp://localhost:8080\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/noah/Desktop/courses/8201/Sandman/Notebooks/wandb/run-20251224_040252-wog6tubo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/noah/sandman-diffusion/runs/wog6tubo' target=\"_blank\">region_diffusion_poisson</a></strong> to <a href='http://localhost:8080/noah/sandman-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/noah/sandman-diffusion' target=\"_blank\">http://localhost:8080/noah/sandman-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/noah/sandman-diffusion/runs/wog6tubo' target=\"_blank\">http://localhost:8080/noah/sandman-diffusion/runs/wog6tubo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 4.59 million parameters.\n"
     ]
    }
   ],
   "source": [
    "# model_args = {\n",
    "#     \"d_model\": 8,        # enough to encode parity + region ID\n",
    "#     \"n_heads\": 1,        # single head is sufficient\n",
    "#     \"n_layers\": 1,       # one transformer block\n",
    "#     \"K\": 1,              # ONE channel per region per time\n",
    "#     \"max_neurons\": 8,    # exact\n",
    "#     \"max_regions\": 3,    # exact\n",
    "#     \"max_eids\": 1,       # exact\n",
    "#     \"use_eid\": False,    # eid carries no information here\n",
    "# }\n",
    "\n",
    "model_args = {\n",
    "    # -------- Region STATE dimension --------\n",
    "    # This is the entire latent the diffusion model operates on.\n",
    "    # Previously this was \"per channel\"; now it is the full region state.\n",
    "    \"d_model\": 128,\n",
    "\n",
    "    # -------- Diffusion backbone --------\n",
    "    \"n_layers\": 8,\n",
    "    \"n_heads\": 8,\n",
    "\n",
    "    # -------- Local encoder pooling --------\n",
    "    # Kenc = number of local pooling queries per region-time\n",
    "    # These DO NOT enter diffusion; they are just an encoder bottleneck.\n",
    "    \"K\": 4,\n",
    "\n",
    "    # -------- Identity / scale parameters --------\n",
    "    \"max_neurons\": 4096,\n",
    "    \"max_regions\": 512,\n",
    "    \"max_eids\": 4096,\n",
    "    \"use_eid\": True,\n",
    "\n",
    "    # -------- Conditioning --------\n",
    "    # Controls AdaLN conditioning width; small is fine.\n",
    "    \"cond_dim\": 64,\n",
    "\n",
    "    # -------- Decoder identity width --------\n",
    "    # Width of neuron identity embedding before hypernetwork.\n",
    "    # Smaller than d_model is good.\n",
    "    \"d_neur_id\": 64,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "noise_scheduler_args = {\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"beta_start\": 1e-4,\n",
    "    \"beta_end\": 0.02,\n",
    "    \"beta_schedule\": \"linear\",\n",
    "}\n",
    "\n",
    "diffusion, optimizer, lr_scheduler, accelerator, data_loader = prepare_data_and_model(\n",
    "        data_loader=data_loader,\n",
    "        model_args=model_args,\n",
    "        noise_scheduler_args=noise_scheduler_args,\n",
    "        target_spec=target_spec,\n",
    "        masking_policy=masking_policy,\n",
    "        max_epochs=50,\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-4,\n",
    "        num_warmup_steps=200,\n",
    "        wandb_enabled=True,\n",
    "        reconstruct_loss_weight=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ea38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 112])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(data_loader[\"train\"]))\n",
    "print(sample_batch['spikes_data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:37<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] avg loss = 3.816432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:35<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] avg loss = 2.946109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:37<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 002] avg loss = 2.701451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:35<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 003] avg loss = 2.654687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [00:34<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 004] avg loss = 2.613941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:17<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] avg loss = 4.397157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:39<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] avg loss = 4.357480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:21<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 002] avg loss = 3.854009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:24<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 003] avg loss = 3.689104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:19<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 004] avg loss = 3.707832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:19<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 005] avg loss = 3.180093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:23<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 006] avg loss = 2.636528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [01:17<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 007] avg loss = 2.355665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 624/787 [01:03<00:15, 10.58it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "diffusion.set_stage(\"ae\") # joint, diffusion, ae\n",
    "for epoch in range(num_epochs):\n",
    "        avg_loss = train_epoch(\n",
    "            loader=data_loader[\"train\"],\n",
    "            diffusion=diffusion,\n",
    "            optimizer=optimizer,\n",
    "            accelerator=accelerator,\n",
    "            epoch=epoch,\n",
    "            log_every=10,\n",
    "            wandb_enabled=True,\n",
    "        )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"[Epoch {epoch:03d}] avg loss = {avg_loss:.6f}\")\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "diffusion.set_stage(\"diffusion\") # joint, diffusion, ae\n",
    "for epoch in range(num_epochs):\n",
    "        avg_loss = train_epoch(\n",
    "            loader=data_loader[\"train\"],\n",
    "            diffusion=diffusion,\n",
    "            optimizer=optimizer,\n",
    "            accelerator=accelerator,\n",
    "            epoch=epoch,\n",
    "            log_every=10,\n",
    "            wandb_enabled=True,\n",
    "        )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"[Epoch {epoch:03d}] avg loss = {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51199556",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This is where you start to wonder what you set reconstruction loss to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sandman.models.sampling import sample_with_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = infer_batch(\n",
    "#     diffusion=diffusion,\n",
    "#     batch=sample_batch,\n",
    "#     accelerator=accelerator,\n",
    "#     masking_policy=masking_policy,\n",
    "#     use_diffusion=True,\n",
    "# )  # [B,T,N]\n",
    "\n",
    "output = sample_with_diffusion(\n",
    "    diffusion=diffusion,\n",
    "    batch=sample_batch,\n",
    "    accelerator=accelerator,\n",
    "    masking_policy=masking_policy,\n",
    "    num_steps=1000,\n",
    ")  # [B,T,N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb13b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 400, 112])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd08305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False,  True,  ..., False, False, False],\n",
      "         [False, False,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "pred = softplus(output)\n",
    "# pred = output\n",
    "print(pred>0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ffbbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_path = \u001b[43mDATA_DIR\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mrnn_example_model.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m torch.save(diffusion.state_dict(), model_path)\n\u001b[32m      3\u001b[39m other_info = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_args\u001b[39m\u001b[33m\"\u001b[39m: model_args,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnoise_scheduler_args\u001b[39m\u001b[33m\"\u001b[39m: noise_scheduler_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreconstruct_loss_weight\u001b[39m\u001b[33m\"\u001b[39m: diffusion.reconstruct_loss_weight,\n\u001b[32m      9\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "if save_model := True:\n",
    "    model_path = DATA_DIR / \"models\" / \"rnn_example_model.pth\"\n",
    "    torch.save(diffusion.state_dict(), model_path)\n",
    "    other_info = {\n",
    "        \"model_args\": model_args,\n",
    "        \"noise_scheduler_args\": noise_scheduler_args,\n",
    "        \"target_spec\": target_spec,\n",
    "        \"masking_policy\": masking_policy,\n",
    "        \"reconstruct_loss_weight\": diffusion.reconstruct_loss_weight,\n",
    "    }\n",
    "    torch.save(other_info, DATA_DIR / \"models\" / \"rnn_example_model_info.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a8ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sandman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
