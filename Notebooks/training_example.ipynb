{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f453f5",
   "metadata": {},
   "source": [
    "# Full Test of imported model\n",
    "\n",
    "Here we run through a full test of the imported model, including loading the model, defining the input data, and running the model to get the output. We will also include some additional checks to ensure the model is working correctly. This is a hard task to train on and it's worth noting that the model will not appear to be working correctly AT ALL. \n",
    "\n",
    "In order to convince you that the model is actually working, try out the blinking dataset, which has neurons that are blinking at different frequencies. Furthermore if you believe that the problem is that the model does diffusion, there is a secret ```disable_diffusion``` flag that you can use to disable the diffusion process. I don't want people shooting themselves in the foot so the flag must be enabled manually after a ```DifusssionWrapper``` is instantiated.\n",
    "\n",
    "Disabling Weights and Biases logging requires setting it to false in ```prepare_data_and_model``` and ```train_epoch```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573adca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sandman.data_loading.data_loader_map import make_map_loader\n",
    "from sandman.data_loading.chaotic_rnn_loader import make_chaotic_rnn_loader\n",
    "from sandman.data_loading.blinking import make_blinking_toy_loader\n",
    "from sandman.data_loading.data_loader_ibl import make_ibl_loader\n",
    "from sandman.paths import DATA_DIR\n",
    "\n",
    "from sandman.models.training import train_epoch, prepare_data_and_model\n",
    "\n",
    "from sandman.models.utils import TargetSpec, MaskingPolicy\n",
    "from sandman.models.utils import SpikeCountPoissonTarget, SpikeCountMSETarget, mask_one_region, mask_one_region_some_times\n",
    "\n",
    "from sandman.models.sampling import sample_region_latent_ddpm\n",
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b3c61",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "Only uncomment one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264e597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== Import of MAP data loading\n",
    "\n",
    "# session_order = pickle.load(open(DATA_DIR / \"tables_and_infos/session_order.pkl\", \"rb\"))\n",
    "# eids = np.sort(session_order[:5]) # originall :40\n",
    "# print(\"Using eids:\", eids)\n",
    "\n",
    "# data_loader, num_neurons, datasets, areaoi_ind, area_ind_list_list, heldout_info_list, trial_type_dict = make_map_loader(\n",
    "#     eids,\n",
    "#     batch_size=2,\n",
    "#     include_opto=False\n",
    "# )\n",
    "\n",
    "# ========================== Import of IBL data loading\n",
    "\n",
    "# with open(DATA_DIR / \"tables_and_infos/ibl_eids.txt\") as file:\n",
    "#     eids = [line.rstrip() for line in file]\n",
    "\n",
    "# eids = eids[:2]\n",
    "# data_loader, num_neurons, datasets, areaoi_ind, area_ind_list_list, heldout_info_list, trial_type_dict = make_ibl_loader(\n",
    "#     eids,\n",
    "#     batch_size=1)\n",
    "\n",
    "#========================== Import of Chaotic RNN data loading\n",
    "\n",
    "# eids = np.arange(5)\n",
    "# data_loader, num_neurons, _, area_ind_list_list, record_info_list = make_chaotic_rnn_loader(\n",
    "#     eids,\n",
    "#     batch_size=1,\n",
    "# )\n",
    "\n",
    "#========================== Import of Blinking Toy data loading\n",
    "\n",
    "data_loader = make_blinking_toy_loader(T=100, batch_size=1, device=\"mps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3125d",
   "metadata": {},
   "source": [
    "### Preview of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c77e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes_data: torch.Size([1, 100, 8])\n",
      "The first (at most) 10 elements along each dimension of the spikes_data tensor are:\n",
      "tensor([[[1., 0., 0., 1., 1., 0., 0., 1.],\n",
      "         [0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "         [1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "         [0., 1., 1., 0., 0., 1., 1., 0.],\n",
      "         [1., 0., 0., 1., 1., 0., 0., 1.],\n",
      "         [0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "         [1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "         [0., 1., 1., 0., 0., 1., 1., 0.],\n",
      "         [1., 0., 0., 1., 1., 0., 0., 1.],\n",
      "         [0., 1., 0., 1., 0., 1., 0., 1.]]], device='mps:0')\n",
      "\n",
      "neuron_regions: torch.Size([1, 8])\n",
      "The first (at most) 10 elements along each dimension of the neuron_regions tensor are:\n",
      "tensor([[0, 0, 1, 1, 2, 2, 2, 2]], device='mps:0')\n",
      "\n",
      "eid: torch.Size([1])\n",
      "The first (at most) 10 elements along each dimension of the eid tensor are:\n",
      "tensor([0], device='mps:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(data_loader['train']))\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"{key}: {value.shape}\")\n",
    "    print(f\"The first (at most) 10 elements along each dimension of the {key} tensor are:\")\n",
    "    new_shape = tuple(slice(0, min(dim, 10)) for dim in value.shape)\n",
    "    print(value[new_shape])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7f46e",
   "metadata": {},
   "source": [
    "### Training/Model Objective!\n",
    "\n",
    "I'm proud of these. This is a generalization of the neural activation modelling process. Play around with the multitude I made in the ```sandman.models.utils``` module. Make your own too! They only take a couple of lines to make!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9045fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = SpikeCountMSETarget()\n",
    "masking_policy = mask_one_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de52067",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Good luck! It's tough to make something that works well. You are going to have a lot of ```n_neurons x d_model``` matrices for encoding and decoding so parameter count is mostly dependent on ```d_model```. Don't let it drop below 16 or it will fundementally break RoPE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5e84bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/noah/Desktop/courses/8201/Sandman/Notebooks/wandb/run-20251220_220424-228n4gdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/noah/sandman-diffusion/runs/228n4gdf' target=\"_blank\">region_diffusion_poisson</a></strong> to <a href='http://localhost:8080/noah/sandman-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/noah/sandman-diffusion' target=\"_blank\">http://localhost:8080/noah/sandman-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/noah/sandman-diffusion/runs/228n4gdf' target=\"_blank\">http://localhost:8080/noah/sandman-diffusion/runs/228n4gdf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 0.60 million parameters.\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    \"d_model\": 64,\n",
    "    \"n_layers\": 8,\n",
    "    \"n_heads\": 16,\n",
    "}\n",
    "\n",
    "noise_scheduler_args = {\n",
    "    \"num_train_timesteps\": 100,\n",
    "    \"beta_start\": 1e-4,\n",
    "    \"beta_end\": 0.02,\n",
    "    \"beta_schedule\": \"linear\",\n",
    "}\n",
    "\n",
    "diffusion, optimizer, lr_scheduler, accelerator, data_loader = prepare_data_and_model(\n",
    "    data_loader=data_loader,\n",
    "    model_args=model_args,\n",
    "    noise_scheduler_args=noise_scheduler_args,\n",
    "    max_epochs=100,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    target_spec=target,\n",
    "    masking_policy=masking_policy,\n",
    "    reconstruct_loss_weight=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ea38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 8])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(data_loader[\"train\"]))\n",
    "print(sample_batch['spikes_data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e9751e4fb24974bba9a71a84333ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 1.9944816876649856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cc14b21e2546679fcb624afa360c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 1.9946530561447144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efba9f5e917413d8a5268efab148a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 1.9924277589321135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c2b308c3914662b01ada2760e23305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 1.9936228736639023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287fc4a52672497fb120920d7930d713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 1.9915680581331252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea57056249f40eda479bcfdd96e6a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 1.9917154278755187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fdda9f8cb4479db415a80f38b33809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        loader=data_loader[\"train\"],\n",
    "        diffusion=diffusion,\n",
    "        optimizer=optimizer,\n",
    "        accelerator=accelerator,\n",
    "        epoch=epoch,\n",
    "        log_every=5 # Lots of logging for debugging purposes\n",
    "    )\n",
    "    print(f\"Epoch {epoch} train loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51199556",
   "metadata": {},
   "source": [
    "### Inference\n",
    "This is where you start to wonder what you set reconstruction loss to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ac2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full, x0_latent = sample_region_latent_ddpm(\n",
    "    model=diffusion.model,\n",
    "    masking_policy=masking_policy,\n",
    "    scheduler=DDPMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_schedule=\"linear\",  # or cosine if you trained with that\n",
    "    ),\n",
    "    batch=sample_batch,\n",
    "    num_inference_steps=20,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd08305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [False,  True, False, False, False, False, False, False],\n",
      "         [ True, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False]]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "pred = softplus(pred_full)\n",
    "print(pred>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ba992",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "\n",
    "The above result is completely amazing. I trained for 10 epochs, then turn reconstruction to 1 and trained for another 2 epochs. The model clearly learned to reconstruct the input data, but is failing on one region. I must check if this is a problem with the masking or the model architecture, or the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ffbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sandman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
